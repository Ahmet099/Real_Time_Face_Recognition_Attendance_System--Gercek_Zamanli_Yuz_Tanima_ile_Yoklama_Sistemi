import os
import tensorflow as tf
import numpy as np
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import cv2
from tensorflow.keras.preprocessing.image import img_to_array
import pickle
import attendance_manager
from sklearn.utils.class_weight import compute_class_weight

def ensure_directories():
    if not os.path.exists("data/train"):
        os.makedirs("data/train")
        print("ğŸ“ KlasÃ¶r oluÅŸturuldu: data/train")

def train_model():
    ensure_directories()
    
    img_width, img_height = 96, 96  # Daha fazla detay iÃ§in 96x96
    batch_size = 32
    epochs = 20

    train_data_dir = 'data/train'
    
    # Training verisinin %20'sini validation iÃ§in ayÄ±rÄ±yoruz.
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=40,
        width_shift_range=0.3,
        height_shift_range=0.3,
        horizontal_flip=True,
        validation_split=0.2
    )

    train_generator = train_datagen.flow_from_directory(
        train_data_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode='categorical',
        subset='training'
    )

    validation_generator = train_datagen.flow_from_directory(
        train_data_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode='categorical',
        subset='validation'
    )
    
    # SÄ±nÄ±f aÄŸÄ±rlÄ±klarÄ±nÄ± hesaplayarak veri dengesizliÄŸini azaltÄ±yoruz.
    labels = train_generator.classes
    class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)
    class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}
    print("ğŸ“Š SÄ±nÄ±f aÄŸÄ±rlÄ±klarÄ±:", class_weights_dict)

    base_model = MobileNetV2(weights="imagenet", include_top=False, input_shape=(img_width, img_height, 3))
    
    model = Sequential([
        base_model,
        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(train_generator.num_classes, activation='softmax')
    ])
    
    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    
    print("ğŸ“Œ Model eÄŸitilmeye baÅŸladÄ±...")
    history = model.fit(
        train_generator,
        epochs=epochs,
        validation_data=validation_generator,
        class_weight=class_weights_dict
    )
    
    model.save('eye_recognition_model.h5')
    print("âœ… Model eÄŸitildi ve 'eye_recognition_model.h5' dosyasÄ± kaydedildi.")
    
    mapping = train_generator.class_indices
    with open("class_indices.pkl", "wb") as f:
        pickle.dump(mapping, f)
    print("ğŸ“ SÄ±nÄ±f eÅŸlemesi kaydedildi:", mapping)

def predict_from_camera():
    if not os.path.exists('eye_recognition_model.h5'):
        print("âš ï¸ Model dosyasÄ± bulunamadÄ±. LÃ¼tfen Ã¶nce modeli eÄŸitmek iÃ§in train_model() fonksiyonunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
        return

    model = tf.keras.models.load_model('eye_recognition_model.h5')
    
    if os.path.exists("class_indices.pkl"):
        with open("class_indices.pkl", "rb") as f:
            mapping = pickle.load(f)
        rev_mapping = {v: k for k, v in mapping.items()}
    else:
        rev_mapping = {}

    cap = cv2.VideoCapture(0)
    print("ğŸ” GerÃ§ek zamanlÄ± tanÄ±ma baÅŸladÄ±. Ã‡Ä±kmak iÃ§in 'q' tuÅŸuna basÄ±n.")
    
    threshold = 0.95  # GÃ¼ven eÅŸiÄŸi

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_eye.xml")
        eyes = eye_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=6)

        for (ex, ey, ew, eh) in eyes:
            cv2.rectangle(frame, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)
            eye_region = frame[ey:ey+eh, ex:ex+ew]
            eye_region = cv2.resize(eye_region, (96, 96))
            eye_region = img_to_array(eye_region) / 255.0
            eye_region = np.expand_dims(eye_region, axis=0)

            predictions = model.predict(eye_region)
            confidence = np.max(predictions)
            if confidence < threshold:
                student_id = "unknown"
            else:
                predicted_class = np.argmax(predictions)
                student_id = rev_mapping.get(predicted_class, "unknown")

            cv2.putText(frame, f"Prediction: {student_id} ({confidence:.2f})", (ex, ey-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)

            if student_id == "unknown":
                cv2.putText(frame, "KayÄ±t bulunamadÄ±, lÃ¼tfen kaydedin.", (ex, ey+eh+20),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255), 2)
            else:
                attendance_manager.add_attendance_record(student_id)

        cv2.imshow("Real-Time Eye Recognition", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    # AÅŸaÄŸÄ±daki satÄ±rlardan ihtiyacÄ±nÄ±za gÃ¶re birini aktif edin.
    # Modeli eÄŸitmek iÃ§in:
    # train_model()
    
    # GerÃ§ek zamanlÄ± tanÄ±ma iÃ§in:
    predict_from_camera()